{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow 实现DQN\n",
    "https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/405_DQN_reinforcement_learning.py\n",
    "\n",
    "more on RL/DQN:\n",
    "https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 2000      # 记忆库大小\n",
    "MEMORY_COUNTER = 0          # for store experience\n",
    "LEARNING_STEP_COUNTER = 0   # for target updating\n",
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "\n",
    "# 以上和pytorch 相同\n",
    "MEMORY = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf placeholders\n",
    "tf_s = tf.placeholder(tf.float32, [None, N_STATES])\n",
    "tf_a = tf.placeholder(tf.int32, [None, ])\n",
    "tf_r = tf.placeholder(tf.float32, [None, ])\n",
    "tf_s_ = tf.placeholder(tf.float32, [None, N_STATES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('q'):        # evaluation network\n",
    "    l_eval = tf.layers.dense(tf_s, 10, tf.nn.relu, kernel_initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    q = tf.layers.dense(l_eval, N_ACTIONS, kernel_initializer=tf.random_normal_initializer(0, 0.1))\n",
    "\n",
    "with tf.variable_scope('q_next'):   # target network, not to train\n",
    "    l_target = tf.layers.dense(tf_s_, 10, tf.nn.relu, trainable=False)\n",
    "    q_next = tf.layers.dense(l_target, N_ACTIONS, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_target = tf_r + GAMMA * tf.reduce_max(q_next, axis=1)                   # shape=(None, ),\n",
    "\n",
    "a_indices = tf.stack([tf.range(tf.shape(tf_a)[0], dtype=tf.int32), tf_a], axis=1)\n",
    "q_wrt_a = tf.gather_nd(params=q, indices=a_indices)     # shape=(None, ), q for current state\n",
    "\n",
    "loss = tf.reduce_mean(tf.squared_difference(q_target, q_wrt_a))\n",
    "train_op = tf.train.AdamOptimizer(LR).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_action(s):\n",
    "    s = s[np.newaxis, :]\n",
    "    if np.random.uniform() < EPSILON:\n",
    "        # forward feed the observation and get q value for every actions\n",
    "        actions_value = sess.run(q, feed_dict={tf_s: s})\n",
    "        action = np.argmax(actions_value)\n",
    "    else:\n",
    "        action = np.random.randint(0, N_ACTIONS)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_transition(s, a, r, s_):\n",
    "    global MEMORY_COUNTER\n",
    "    transition = np.hstack((s, [a, r], s_))\n",
    "    # replace the old memory with new memory\n",
    "    index = MEMORY_COUNTER % MEMORY_CAPACITY\n",
    "    MEMORY[index, :] = transition\n",
    "    MEMORY_COUNTER += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn():\n",
    "    # update target net\n",
    "    global LEARNING_STEP_COUNTER\n",
    "    if LEARNING_STEP_COUNTER % TARGET_REPLACE_ITER == 0:\n",
    "        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_next')\n",
    "        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q')\n",
    "        sess.run([tf.assign(t, e) for t, e in zip(t_params, e_params)])\n",
    "    LEARNING_STEP_COUNTER += 1\n",
    "\n",
    "    # learning\n",
    "    sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "    b_memory = MEMORY[sample_index, :]\n",
    "    b_s = b_memory[:, :N_STATES]\n",
    "    b_a = b_memory[:, N_STATES].astype(int)\n",
    "    b_r = b_memory[:, N_STATES+1]\n",
    "    b_s_ = b_memory[:, -N_STATES:]\n",
    "    sess.run(train_op, {tf_s: b_s, tf_a: b_a, tf_r: b_r, tf_s_: b_s_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting experience...\n",
      "Ep:  199 | Ep_r:  2.91\n",
      "Ep:  200 | Ep_r:  1.1\n",
      "Ep:  201 | Ep_r:  1.91\n",
      "Ep:  202 | Ep_r:  18.52\n",
      "Ep:  203 | Ep_r:  0.92\n",
      "Ep:  204 | Ep_r:  2.29\n",
      "Ep:  205 | Ep_r:  4.31\n",
      "Ep:  206 | Ep_r:  1.91\n",
      "Ep:  207 | Ep_r:  8.75\n",
      "Ep:  208 | Ep_r:  1.04\n",
      "Ep:  209 | Ep_r:  13.4\n",
      "Ep:  210 | Ep_r:  21.75\n",
      "Ep:  211 | Ep_r:  20.24\n",
      "Ep:  212 | Ep_r:  31.85\n",
      "Ep:  213 | Ep_r:  14.58\n",
      "Ep:  214 | Ep_r:  51.82\n",
      "Ep:  215 | Ep_r:  64.51\n",
      "Ep:  216 | Ep_r:  13.48\n",
      "Ep:  217 | Ep_r:  23.54\n",
      "Ep:  218 | Ep_r:  788.64\n",
      "Ep:  219 | Ep_r:  53.05\n",
      "Ep:  220 | Ep_r:  139.3\n",
      "Ep:  221 | Ep_r:  1248.8\n",
      "Ep:  222 | Ep_r:  77.1\n",
      "Ep:  223 | Ep_r:  123.41\n",
      "Ep:  224 | Ep_r:  143.41\n",
      "Ep:  225 | Ep_r:  126.96\n",
      "Ep:  226 | Ep_r:  116.76\n",
      "Ep:  227 | Ep_r:  209.43\n",
      "Ep:  228 | Ep_r:  446.55\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-640c8a9d1b66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mep_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Think\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Think\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Think\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Think\\Anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'flip'"
     ]
    }
   ],
   "source": [
    "print('\\nCollecting experience...')\n",
    "for i_episode in range(400):\n",
    "    s = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        a = choose_action(s)\n",
    "\n",
    "        # take action\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        # modify the reward\n",
    "        x, x_dot, theta, theta_dot = s_\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        r = r1 + r2\n",
    "\n",
    "        store_transition(s, a, r, s_)\n",
    "\n",
    "        ep_r += r\n",
    "        if MEMORY_COUNTER > MEMORY_CAPACITY:\n",
    "            learn()\n",
    "            if done:\n",
    "                print('Ep: ', i_episode,\n",
    "                      '| Ep_r: ', round(ep_r, 2))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        s = s_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下为tensorflow的 class 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n",
      "[  4.80000000e+00   3.40282347e+38   4.18879020e-01   3.40282347e+38]\n",
      "[ -4.80000000e+00  -3.40282347e+38  -4.18879020e-01  -3.40282347e+38]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# from RL_brain import DeepQNetwork\n",
    "\n",
    "# 定义env\n",
    "env = gym.make('CartPole-v0')   # 定义使用 gym 库中的那一个环境\n",
    "env = env.unwrapped # 不做这个会有很多限制\n",
    "\n",
    "print(env.action_space) # 查看这个环境中可用的 action 有多少个\n",
    "print(env.observation_space)    # 查看这个环境中可用的 state 的 observation 有多少个\n",
    "print(env.observation_space.high)   # 查看 observation 最高取值\n",
    "print(env.observation_space.low)    # 查看 observation 最低取值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义class\n",
    "\n",
    "#### 1. 框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepQNetwork:\n",
    "    # 上次的内容\n",
    "    def _build_net(self):\n",
    "\n",
    "    # 这次的内容:\n",
    "    # 初始值\n",
    "    def __init__(self):\n",
    "\n",
    "    # 存储记忆\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "\n",
    "    # 选行为\n",
    "    def choose_action(self, observation):\n",
    "\n",
    "    # 学习\n",
    "    def learn(self):\n",
    "\n",
    "    # 看看学习效果 (可选)\n",
    "    def plot_cost(self):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.具体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepQNetwork:\n",
    "    def _build_net(self):\n",
    "        \"\"\"\n",
    "        创建两个网络:两个神经网络是为了固定住一个神经网络 (target_net) 的参数, \n",
    "                    target_net 是 eval_net 的一个历史版本, 拥有 eval_net 很久之前的一组参数, \n",
    "                            而且这组参数被固定一段时间, 然后再被 eval_net 的新参数所替换. \n",
    "                    而 eval_net 是不断在被提升的, 所以是一个可以被训练的网络 trainable=True. \n",
    "                    而 target_net 的 trainable=False.\n",
    "        \"\"\"\n",
    "        # -------------- 创建 eval 神经网络, 及时提升参数 --------------\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # 用来接收 observation\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target') # 用来接收 q_target 的值, 这个之后会通过计算得到\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            # c_names(collections_names) 是在更新 target_net 参数时会用到\n",
    "            c_names, n_l1, w_initializer, b_initializer = \\\n",
    "                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 10, \\\n",
    "                tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers\n",
    "\n",
    "            # eval_net 的第一层. collections 是在更新 target_net 参数时会用到\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n",
    "\n",
    "            # eval_net 的第二层. collections 是在更新 target_net 参数时会用到\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_eval = tf.matmul(l1, w2) + b2\n",
    "\n",
    "        with tf.variable_scope('loss'): # 求误差\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        with tf.variable_scope('train'):    # 梯度下降\n",
    "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        # ---------------- 创建 target 神经网络, 提供 target Q ---------------------\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    # 接收下个 observation\n",
    "        with tf.variable_scope('target_net'):\n",
    "            # c_names(collections_names) 是在更新 target_net 参数时会用到\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            # target_net 的第一层. collections 是在更新 target_net 参数时会用到\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)\n",
    "\n",
    "            # target_net 的第二层. collections 是在更新 target_net 参数时会用到\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                self.q_next = tf.matmul(l1, w2) + b2\n",
    "                \n",
    "                \n",
    "    def __init__(self, n_actions, n_features, learning_rate=0.01,\n",
    "            reward_decay=0.9,  e_greedy=0.9,  replace_target_iter=300,\n",
    "            memory_size=500,   batch_size=32, e_greedy_increment=None, output_graph=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy     # epsilon 的最大值\n",
    "        self.replace_target_iter = replace_target_iter  # 更换 target_net 的步数\n",
    "        self.memory_size = memory_size  # 记忆上限\n",
    "        self.batch_size = batch_size    # 每次更新时从 memory 里面取多少记忆出来\n",
    "        self.epsilon_increment = e_greedy_increment # epsilon 的增量\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max # 是否开启探索模式, 并逐步减少探索次数\n",
    "\n",
    "        # 记录学习次数 (用于判断是否更换 target_net 参数)\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # 初始化全 0 记忆 [s, a, r, s_]\n",
    "        self.memory = np.zeros((self.memory_size, n_features*2+2)) # 和视频中不同, 因为 pandas 运算比较慢, 这里改为直接用 numpy\n",
    "\n",
    "        # 创建 [target_net, evaluate_net]\n",
    "        self._build_net()\n",
    "\n",
    "        # 替换 target net 的参数\n",
    "        t_params = tf.get_collection('target_net_params')  # 提取 target_net 的参数\n",
    "        e_params = tf.get_collection('eval_net_params')   # 提取  eval_net 的参数\n",
    "        self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)] # 更新 target_net 参数\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        # 输出 tensorboard 文件\n",
    "        if output_graph:\n",
    "            # $ tensorboard --logdir=logs\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.cost_his = []  # 记录所有 cost 变化, 用于最后 plot 出来观看\n",
    "    \n",
    "    \n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        \"\"\"\n",
    "        存储记忆：记录下所有经历过的步, 这些步可以进行反复的学习, \n",
    "                所以是一种 off-policy 方法, 你甚至可以自己玩, \n",
    "                然后记录下自己玩的经历, 让这个 DQN 学习你是如何通关的.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "\n",
    "        # 记录一条 [s, a, r, s_] 记录\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "\n",
    "        # 总 memory 大小是固定的, 如果超出总大小, 旧 memory 就被新 memory 替换\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index, :] = transition # 替换过程\n",
    "\n",
    "        self.memory_counter += 1\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        \"\"\"\n",
    "        选行为: 和之前的 QLearningTable, SarsaTable 等一样, 都需要一个选行为的功能.\n",
    "        \"\"\"\n",
    "        # 统一 observation 的 shape (1, size_of_observation)\n",
    "        observation = observation[np.newaxis, :]\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # 让 eval_net 神经网络生成所有 action 的值, 并选择值最大的 action\n",
    "            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)   # 随机选择\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        学习: 更新参数的. 这里涉及了 target_net 和 eval_net 的交互使用.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 检查是否替换 target_net 参数\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.replace_target_op)\n",
    "            print('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        # 从 memory 中随机抽取 batch_size 这么多记忆\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        # 获取 q_next (target_net 产生了 q) 和 q_eval(eval_net 产生的 q)\n",
    "        q_next, q_eval = self.sess.run(\n",
    "            [self.q_next, self.q_eval],\n",
    "            feed_dict={\n",
    "                self.s_: batch_memory[:, -self.n_features:],\n",
    "                self.s: batch_memory[:, :self.n_features]\n",
    "            })\n",
    "\n",
    "        # 下面这几步十分重要. q_next, q_eval 包含所有 action 的值,\n",
    "        # 而我们需要的只是已经选择好的 action 的值, 其他的并不需要.\n",
    "        # 所以我们将其他的 action 值全变成 0, 将用到的 action 误差值 反向传递回去, 作为更新凭据.\n",
    "        # 这是我们最终要达到的样子, 比如 q_target - q_eval = [1, 0, 0] - [-1, 0, 0] = [2, 0, 0]\n",
    "        # q_eval = [-1, 0, 0] 表示这一个记忆中有我选用过 action 0, 而 action 0 带来的 Q(s, a0) = -1, 所以其他的 Q(s, a1) = Q(s, a2) = 0.\n",
    "        # q_target = [1, 0, 0] 表示这个记忆中的 r+gamma*maxQ(s_) = 1, 而且不管在 s_ 上我们取了哪个 action,\n",
    "        # 我们都需要对应上 q_eval 中的 action 位置, 所以就将 1 放在了 action 0 的位置.\n",
    "\n",
    "        # 下面也是为了达到上面说的目的, 不过为了更方面让程序运算, 达到目的的过程有点不同.\n",
    "        # 是将 q_eval 全部赋值给 q_target, 这时 q_target-q_eval 全为 0,\n",
    "        # 不过 我们再根据 batch_memory 当中的 action 这个 column 来给 q_target 中的对应的 memory-action 位置来修改赋值.\n",
    "        # 使新的赋值为 reward + gamma * maxQ(s_), 这样 q_target-q_eval 就可以变成我们所需的样子.\n",
    "        # 具体在下面还有一个举例说明.\n",
    "\n",
    "        q_target = q_eval.copy()\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        reward = batch_memory[:, self.n_features + 1]\n",
    "\n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1)\n",
    "\n",
    "        \"\"\"\n",
    "        假如在这个 batch 中, 我们有2个提取的记忆, 根据每个记忆可以生产3个 action 的值:\n",
    "        q_eval =\n",
    "        [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "\n",
    "        q_target = q_eval =\n",
    "        [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "\n",
    "        然后根据 memory 当中的具体 action 位置来修改 q_target 对应 action 上的值:\n",
    "        比如在:\n",
    "            记忆 0 的 q_target 计算值是 -1, 而且我用了 action 0;\n",
    "            记忆 1 的 q_target 计算值是 -2, 而且我用了 action 2:\n",
    "        q_target =\n",
    "        [[-1, 2, 3],\n",
    "         [4, 5, -2]]\n",
    "\n",
    "        所以 (q_target - q_eval) 就变成了:\n",
    "        [[(-1)-(1), 0, 0],\n",
    "         [0, 0, (-2)-(6)]]\n",
    "\n",
    "        最后我们将这个 (q_target - q_eval) 当成误差, 反向传递会神经网络.\n",
    "        所有为 0 的 action 值是当时没有选择的 action, 之前有选择的 action 才有不为0的值.\n",
    "        我们只反向传递之前选择的 action 的值,\n",
    "        \"\"\"\n",
    "\n",
    "        # 训练 eval_net\n",
    "        _, self.cost = self.sess.run([self._train_op, self.loss],\n",
    "                                     feed_dict={self.s: batch_memory[:, :self.n_features],\n",
    "                                                self.q_target: q_target})\n",
    "        self.cost_his.append(self.cost) # 记录 cost 误差\n",
    "\n",
    "        # 逐渐增加 epsilon, 降低行为的随机性\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "    def plot_cost(self):\n",
    "        \"\"\"\n",
    "        输出cost曲线的变化\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(np.arange(len(self.cost_his)), self.cost_his)\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('training steps')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 ep_r:  16.94  epsilon:  0\n",
      "episode:  1 ep_r:  9.24  epsilon:  0\n",
      "episode:  2 ep_r:  4.96  epsilon:  0\n",
      "episode:  3 ep_r:  10.13  epsilon:  0\n",
      "episode:  4 ep_r:  3.36  epsilon:  0\n",
      "episode:  5 ep_r:  8.68  epsilon:  0\n",
      "episode:  6 ep_r:  2.8  epsilon:  0\n",
      "episode:  7 ep_r:  5.87  epsilon:  0\n",
      "episode:  8 ep_r:  5.59  epsilon:  0\n",
      "episode:  9 ep_r:  3.06  epsilon:  0\n",
      "episode:  10 ep_r:  3.34  epsilon:  0\n",
      "episode:  11 ep_r:  8.24  epsilon:  0\n",
      "episode:  12 ep_r:  6.18  epsilon:  0\n",
      "episode:  13 ep_r:  3.09  epsilon:  0\n",
      "episode:  14 ep_r:  2.42  epsilon:  0\n",
      "episode:  15 ep_r:  5.74  epsilon:  0\n",
      "episode:  16 ep_r:  3.03  epsilon:  0\n",
      "episode:  17 ep_r:  4.74  epsilon:  0\n",
      "episode:  18 ep_r:  5.52  epsilon:  0\n",
      "episode:  19 ep_r:  2.54  epsilon:  0\n",
      "episode:  20 ep_r:  2.57  epsilon:  0\n",
      "episode:  21 ep_r:  1.99  epsilon:  0\n",
      "episode:  22 ep_r:  7.5  epsilon:  0\n",
      "episode:  23 ep_r:  7.36  epsilon:  0\n",
      "episode:  24 ep_r:  6.82  epsilon:  0\n",
      "episode:  25 ep_r:  4.44  epsilon:  0\n",
      "episode:  26 ep_r:  5.34  epsilon:  0\n",
      "episode:  27 ep_r:  4.29  epsilon:  0\n",
      "episode:  28 ep_r:  9.33  epsilon:  0\n",
      "episode:  29 ep_r:  22.78  epsilon:  0\n",
      "episode:  30 ep_r:  9.66  epsilon:  0\n",
      "episode:  31 ep_r:  5.67  epsilon:  0\n",
      "episode:  32 ep_r:  5.67  epsilon:  0\n",
      "episode:  33 ep_r:  4.63  epsilon:  0\n",
      "episode:  34 ep_r:  4.79  epsilon:  0\n",
      "episode:  35 ep_r:  8.88  epsilon:  0\n",
      "episode:  36 ep_r:  7.32  epsilon:  0\n",
      "episode:  37 ep_r:  7.07  epsilon:  0\n",
      "episode:  38 ep_r:  11.42  epsilon:  0\n",
      "episode:  39 ep_r:  9.67  epsilon:  0\n",
      "episode:  40 ep_r:  5.55  epsilon:  0\n",
      "episode:  41 ep_r:  7.13  epsilon:  0\n",
      "episode:  42 ep_r:  1.27  epsilon:  0\n",
      "episode:  43 ep_r:  5.12  epsilon:  0\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  44 ep_r:  5.61  epsilon:  0.01\n",
      "episode:  45 ep_r:  6.57  epsilon:  0.03\n",
      "episode:  46 ep_r:  10.12  epsilon:  0.06\n",
      "episode:  47 ep_r:  5.13  epsilon:  0.07\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  48 ep_r:  9.62  epsilon:  0.09\n",
      "episode:  49 ep_r:  5.5  epsilon:  0.1\n",
      "episode:  50 ep_r:  7.67  epsilon:  0.12\n",
      "episode:  51 ep_r:  1.89  epsilon:  0.12\n",
      "episode:  52 ep_r:  1.13  epsilon:  0.13\n",
      "episode:  53 ep_r:  8.9  epsilon:  0.15\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  54 ep_r:  7.17  epsilon:  0.17\n",
      "episode:  55 ep_r:  11.72  epsilon:  0.19\n",
      "episode:  56 ep_r:  3.52  epsilon:  0.2\n",
      "episode:  57 ep_r:  1.71  epsilon:  0.22\n",
      "episode:  58 ep_r:  2.73  epsilon:  0.23\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  59 ep_r:  4.91  epsilon:  0.24\n",
      "episode:  60 ep_r:  3.92  epsilon:  0.25\n",
      "episode:  61 ep_r:  15.47  epsilon:  0.29\n",
      "episode:  62 ep_r:  2.78  epsilon:  0.3\n",
      "episode:  63 ep_r:  3.81  epsilon:  0.32\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  64 ep_r:  7.98  epsilon:  0.34\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  65 ep_r:  7.53  epsilon:  0.4\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  66 ep_r:  -2.61  epsilon:  0.49\n",
      "episode:  67 ep_r:  23.35  epsilon:  0.53\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  68 ep_r:  33.8  epsilon:  0.62\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  69 ep_r:  47.58  epsilon:  0.74\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  70 ep_r:  57.84  epsilon:  0.88\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  71 ep_r:  106.82  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  72 ep_r:  26.41  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  73 ep_r:  27.08  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  74 ep_r:  60.02  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  75 ep_r:  171.95  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  76 ep_r:  33.11  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  77 ep_r:  33.77  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  78 ep_r:  63.98  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  79 ep_r:  45.96  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  80 ep_r:  25.91  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  81 ep_r:  22.8  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  82 ep_r:  66.44  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  83 ep_r:  96.51  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  84 ep_r:  53.42  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  85 ep_r:  41.87  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  86 ep_r:  96.56  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  87 ep_r:  121.47  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  88 ep_r:  47.62  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  89 ep_r:  93.54  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  90 ep_r:  51.88  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  91 ep_r:  52.64  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  92 ep_r:  24.97  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  93 ep_r:  108.98  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  94 ep_r:  59.76  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  95 ep_r:  118.66  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  96 ep_r:  169.53  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  97 ep_r:  60.9  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  98 ep_r:  202.35  epsilon:  0.9\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "\n",
      "target_params_replaced\n",
      "\n",
      "episode:  99 ep_r:  71.08  epsilon:  0.9\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.pyplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ccc3b125f06b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtotal_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[1;31m# 最后输出 cost 曲线\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mRL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-e1e7f497b39f>\u001b[0m in \u001b[0;36mplot_cost\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0m输出cost曲线的变化\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \"\"\"\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_his\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_his\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cost'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.pyplot'"
     ]
    }
   ],
   "source": [
    "# 定义使用 DQN 的算法\n",
    "RL = DeepQNetwork(n_actions=env.action_space.n,\n",
    "                  n_features=env.observation_space.shape[0],\n",
    "                  learning_rate=0.01, e_greedy=0.9,\n",
    "                  replace_target_iter=100, memory_size=2000,\n",
    "                  e_greedy_increment=0.0008,)\n",
    "\n",
    "total_steps = 0 # 记录步数\n",
    "\n",
    "for i_episode in range(100):\n",
    "\n",
    "    # 获取回合 i_episode 第一个 observation\n",
    "    observation = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        env.render()    # 刷新环境\n",
    "\n",
    "        action = RL.choose_action(observation)  # 选行为\n",
    "\n",
    "        observation_, reward, done, info = env.step(action) # 获取下一个 state\n",
    "\n",
    "        x, x_dot, theta, theta_dot = observation_   # 细分开, 为了修改原配的 reward\n",
    "\n",
    "        # x 是车的水平位移, 所以 r1 是车越偏离中心, 分越少\n",
    "        # theta 是棒子离垂直的角度, 角度越大, 越不垂直. 所以 r2 是棒越垂直, 分越高\n",
    "\n",
    "        x, x_dot, theta, theta_dot = observation_\n",
    "        r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5\n",
    "        reward = r1 + r2   # 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度, 这样 DQN 学习更有效率\n",
    "\n",
    "        # 保存这一组记忆\n",
    "        RL.store_transition(observation, action, reward, observation_)\n",
    "\n",
    "        if total_steps > 1000:\n",
    "            RL.learn()  # 学习\n",
    "\n",
    "        ep_r += reward\n",
    "        if done:\n",
    "            print('episode: ', i_episode,\n",
    "                  'ep_r: ', round(ep_r, 2),\n",
    "                  ' epsilon: ', round(RL.epsilon, 2))\n",
    "            break\n",
    "\n",
    "        observation = observation_\n",
    "        total_steps += 1\n",
    "# 最后输出 cost 曲线\n",
    "RL.plot_cost()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
